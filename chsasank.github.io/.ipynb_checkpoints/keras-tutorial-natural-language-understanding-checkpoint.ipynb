{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Tutorial - Natural Language Understanding\n",
    "\n",
    "http://chsasank.github.io/spoken-language-understanding.html\n",
    "\n",
    "Natural language understanding (NLU) is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem.\n",
    "The process of disassembling and parsing input is more complex than the reverse process of assembling output in natural language generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when outputting language.[dubious – discuss]\n",
    "There is considerable commercial interest in the field because of its application to news-gathering, text categorization, voice-activation, archiving, and large-scale content-analysis.\n",
    "\n",
    "Problem and Dataset\n",
    "\n",
    "NLU: extract meaning of speech -- still an unsolved problem\n",
    "\n",
    "We breat this problem into a solvable practical problem of understanding the speaker in a limited context.\n",
    "\n",
    "In particular, we want to identify the intent of a speaker asking information about flights\n",
    "\n",
    "Dataset: Airline Travel Information System (ATIS)\n",
    "- Collected by DARPA in early 90s\n",
    "- ATIS consists of spoken queries on flight related information\n",
    "- eg, I want to go from Boston to Atlanta on Monday\n",
    "- Understanding this is then reduced to identifying arguments like Destination and Departure Day\n",
    "- This task is called `slot-filling`\n",
    "\n",
    "Example sentence:\n",
    "\n",
    "|Words|Show|flights|from|Boston|to|New|York|today|\n",
    "|Labels|O|O|O|B-dept|O|B-arr|I-arr|B-date|\n",
    "\n",
    "ATIS contains 5M sentences for total of 56,590 / 9,198 words (avg sentence length is 15) in the train / test set.\n",
    "\n",
    "Number of classes (different slots) is 128 including the O label (NULL)\n",
    "\n",
    "Unseen words in the test set are encoded `<UNK>` and each digit is replaced with string `DIGIT`, ie 20 is convert to `DIGITDIGIT`\n",
    "\n",
    "Our approach is to use:\n",
    "- Word embeddings\n",
    "- Recurrent neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "\n",
    "Word embeddings map words to a vector in a high-dimensional space.  \n",
    "\n",
    "If learned the right way, these word embeddings can learn semantic and syntactic information of the words, ie, similar words are close to each other in this space and dissimilar words are far apart.\n",
    "\n",
    "These can be learned either using large amounts of text like Wikipedia or specifically for a given problem.  We will take the second approach.\n",
    "\n",
    "Eg, nearest neighbords in the word embedding space for some of the words\n",
    "\n",
    "sunday\tdelta\tcalifornia\tboston\taugust\ttime\tcar\n",
    "wednesday\tcontinental\tcolorado\tnashville\tseptember\tschedule\trental\n",
    "saturday\tunited\tflorida\ttoronto\tjuly\ttimes\tlimousine\n",
    "friday\tamerican\tohio\tchicago\tjune\tschedules\trentals\n",
    "monday\teastern\tgeorgia\tphoenix\tdecember\tdinnertime\tcars\n",
    "tuesday\tnorthwest\tpennsylvania\tcleveland\tnovember\tord\ttaxi\n",
    "thursday\tus\tnorth\tatlanta\tapril\tf28\ttrain\n",
    "wednesdays\tnationair\ttennessee\tmilwaukee\toctober\tlimo\tlimo\n",
    "saturdays\tlufthansa\tminnesota\tcolumbus\tjanuary\tdeparture\tap\n",
    "sundays\tmidwest\tmichigan\tminneapolis\tmay\tsfo\tlater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN)\n",
    "\n",
    "Convolutional layers can be a great way to pool local information, but they don't really capture the sequentiality of the data.\n",
    "\n",
    "RNNs help us tackle sequential information like natural language.\n",
    "\n",
    "If we're going to predict properties of the current word, we better remember the words before it too.  \n",
    "\n",
    "An RNN has such an internal state / memory which stores the summary of the sequence it has seen so far.\n",
    "\n",
    "This allows the the RNN to solve complicated word tagging problems like part of speech (POS) tagging or slot filling as in our case.\n",
    "\n",
    "Diagram:\n",
    "\n",
    "![RNN][rnn-diagram]\n",
    "[rnn-diagram]: http://chsasank.github.io/assets/images/slu/rnn.gif \"RNN\"\n",
    "\n",
    "- x1, x2, ..., xt-1, xt, xt+1.. is the input to RNN\n",
    "- s_t is the hidden state of RNN, calculated based on state at step t-1\n",
    "- s_t = f(Ux_t + s_t-1) / f is nonlinearity like tanh or ReLU\n",
    "- o_t is the output at step t.  Computed as o_t = F(Vs_t)\n",
    "- U, V, W are the learnable parameters of RNN\n",
    "\n",
    "For our problem, we will pass word embedding sequence as the input to the RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together\n",
    "\n",
    "Since we are using IOB representation for labels, it’s not trivial to calculate the scores of our model. We therefore use the conlleval perl script (http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt) to compute the F1 Scores (https://en.wikipedia.org/wiki/F1_score). I’ve adapted the code from here(https://github.com/mesnilgr/is13) for the data preprocessing and score calculation. Complete code is available at GitHub (https://github.com/chsasank/ATIS.keras)\n",
    "\n",
    "$ git clone https://github.com/chsasank/ATIS.keras.git\n",
    "$ cd ATIS.keras\n",
    "\n",
    "I recommend using jupyter notebook to run and experiment with the snippets from the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "# load data using data.load.atisfull() -- will download data first time it is run\n",
    "# words and labels are encoded as indices to a vocabulary\n",
    "# this vocabulary is stored in w2idx and labels2idx\n",
    "\n",
    "import numpy as np\n",
    "import data.load\n",
    "\n",
    "train_set, valid_set, dicts = data.load.atisfull()\n",
    "w2idx, labels2idx = dicts['words2idx'], dicts['labels2idx']\n",
    "\n",
    "train_x, _, train_label = train_set\n",
    "val_x, _, val_label = valid_set\n",
    "\n",
    "# create index to word / label dicts\n",
    "idx2w = {w2idx[k]:k for k in w2idx}\n",
    "idx2la = {labels2idx[k]:k for k in labels2idx}\n",
    "\n",
    "# for conlleval script\n",
    "words_train = [ list(map(lambda x: idx2w[x], w)) for w in train_x]\n",
    "labels_train = [ list(map(lambda x: idx2la[x], y)) for y in train_label]\n",
    "words_val = [ list(map(lambda x: idx2w[x], w)) for w in val_x]\n",
    "labels_val = [ list(map(lambda x: idx2la[x], y)) for y in val_label]\n",
    "\n",
    "n_classes = len(idx2la)\n",
    "n_vocab = len(idx2w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence : ['i', 'want', 'to', 'fly', 'from', 'boston', 'at', 'DIGITDIGITDIGIT', 'am', 'and', 'arrive', 'in', 'denver', 'at', 'DIGITDIGITDIGITDIGIT', 'in', 'the', 'morning']\n",
      "Encoded form: [232 542 502 196 208  77  62  10  35  40  58 234 137  62  11 234 481 321]\n",
      "\n",
      "It's label : ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time', 'I-depart_time.time', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-arrive_time.time', 'O', 'O', 'B-arrive_time.period_of_day']\n",
      "Encoded form: [126 126 126 126 126  48 126  35  99 126 126 126  78 126  14 126 126  12]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example sentence : {}\".format(words_train[0]))\n",
    "print(\"Encoded form: {}\".format(train_x[0]))\n",
    "print()\n",
    "print(\"It's label : {}\".format(labels_train[0]))\n",
    "print(\"Encoded form: {}\".format(train_label[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Model\n",
    "\n",
    "Keras has inbuilt `Embedding` layer for word embeddings. It expects integer indices.\n",
    "\n",
    "`SimpleRNN` is the RNN layer described above.  We will have to use `TimeDistrubuted` to pass the output of the RNN $o_t$ at each time step $t$ to a fully connected layer.  Otherwise, output at the final step will be passed on to the next layer.\n",
    "\n",
    "```\n",
    "keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "\n",
    "Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model.\n",
    "\n",
    "model = Sequential()\n",
    "  model.add(Embedding(1000, 64, input_length=10))\n",
    "  # the model will take as input an integer matrix of size (batch, input_length).\n",
    "  # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "  # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "  input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "  model.compile('rmsprop', 'mse')\n",
    "  output_array = model.predict(input_array)\n",
    "  assert output_array.shape == (32, 10, 64)\n",
    "```\n",
    "\n",
    "```\n",
    "keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, 100))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(SimpleRNN(100, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "We will pass each sentence as a batch to the model.\n",
    "\n",
    "We cannot use `model.fit()` as it expects all the sentences to be of the same size.\n",
    "\n",
    "We will therefore use `model.train_on_batch()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:25 Time: 0:05:25\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:11 Time: 0:05:11\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:12 Time: 0:05:12\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:10 Time: 0:05:10\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:10 Time: 0:05:10\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:11 Time: 0:05:11\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:09 Time: 0:05:09\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:10 Time: 0:05:10\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:11 Time: 0:05:11\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:10 Time: 0:05:10\n",
      "100% (4978 of 4978) |#####################| Elapsed Time: 0:05:10 Time: 0:05:10\n",
      " 45% (2281 of 4978) |##########            | Elapsed Time: 0:02:22 ETA: 0:02:49"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "n_epochs = 30\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    bar = progressbar.ProgressBar(max_value=len(train_x))\n",
    "    for n_batch, sent in bar(enumerate(train_x)):\n",
    "        label = train_label[n_batch]\n",
    "        # Make labels one hot\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        # View each sentence as a batch\n",
    "        sent = sent[np.newaxis,:]\n",
    "        \n",
    "        if sent.shape[1] > 1: # ignore 1 word sentences\n",
    "            model.train_on_batch(sent, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "To measure the accuracy of the model, we use `model.predict_on_batch()` and `metrics.accuracy.conlleval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metrics.accuracy import conlleval\n",
    "\n",
    "labels_pred_val = []\n",
    "\n",
    "bar = progressbar.ProgressBar(max_value=len(val_x))\n",
    "for n_batch, sent in bar(enumerate(val_x))\n",
    "    label = val_label[n_batch]\n",
    "    label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "    sent = sent[np.newaxis,:]\n",
    "    \n",
    "    pred = model.predict_on_batch(sent)\n",
    "    pred = np.argmx(pred,-1)[0]\n",
    "    labels_pred_val.append(pred)\n",
    "\n",
    "labels_pred_val = [list(map(lambda x: idx2la[x], y)) \\\n",
    "                                   for y in labels_pred_val]\n",
    "con_dict = conlleval(labels_pred_val, labels_val,\n",
    "                    words_val, 'measure.txt')\n",
    "\n",
    "print('Precision = {}, Recall = {}, F1 = {}'.format(\n",
    "            con_dict['r'], con_dict['p'], con_dict['f1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, I get 92.36 F1 Score.\n",
    "\n",
    "`Precision = 92.07, Recall = 92.66, F1 = 92.36`\n",
    "\n",
    "Note that for the sake of brevity, I’ve not showed logging part of the code. Loggging losses and accuracies is an important part of coding up an model. An improved model (described in the next section) with logging is at main.py. You can run it as :\n",
    "\n",
    "`python main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements\n",
    "\n",
    "One drawback with our current model is that there is no lookahead, ie, the output $o_t$ depends only on the current and previous words but not on the words next to it.  One can imagine that clues about the properties of the current word is also held about the next word.\n",
    "\n",
    "Lookahead can easily be implemented by having a convolutional layer before RNN and after word embeddings:\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab,100))\n",
    "model.add(Convolution1D(128, 5, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(100,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')\n",
    "```\n",
    "With this improved model, I get 94.90 F1 Score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In this tutorial, we have learnt about word embeddings and RNNs. We have applied these to a NLP problem: ATIS. We also have made an improvement to our model.\n",
    "\n",
    "To improve the model further, we could try using word embeddings learnt on a large corpus like Wikipedia. Also, there are variants of RNNs like LSTM or GRU which can be experimented with.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
